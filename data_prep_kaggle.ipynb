{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-07T14:01:18.316387Z",
     "start_time": "2024-04-07T13:57:55.740302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 47994, number of negative: 1478665\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 23489\n",
      "[LightGBM] [Info] Number of data points in the train set: 1526659, number of used features: 20\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031437 -> initscore=-3.427819\n",
      "[LightGBM] [Info] Start training from score -3.427819\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def column_check(data1, data2):\n",
    "    return set(data1.columns) == set(data1.columns).intersection(data2.columns)\n",
    "\n",
    "\n",
    "def merge_duplicate_group_cols(data):\n",
    "    if 'num_group1_x' in data.columns:\n",
    "        data.loc[:, 'num_group1_x'] = data.loc[:, 'num_group1_x'].fillna(data.loc[:, 'num_group1_y'])\n",
    "        data.drop('num_group1_y', axis=1, inplace=True)\n",
    "        data.rename(columns={'num_group1_x': 'num_group1'}, inplace=True)\n",
    "\n",
    "    if 'num_group2_x' in data.columns:\n",
    "        data.loc[:, 'num_group2_x'] = data.loc[:, 'num_group2_x'].fillna(data.loc[:, 'num_group2_y'])\n",
    "        data.drop('num_group2_y', axis=1, inplace=True)\n",
    "        data.rename(columns={'num_group2_x': 'num_group2'}, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def concat_and_merge(base_data, path_list, base_path, rows, cols_to_merge=None):\n",
    "    data = pd.DataFrame()\n",
    "    for file in path_list:\n",
    "        if cols_to_merge:\n",
    "            # I specify whether read all columns or a given list of columns.\n",
    "            data = pd.concat([data, pd.read_csv(base_path + file, usecols=['case_id'] + cols_to_merge, nrows=rows,\n",
    "                                                low_memory=False)], axis=0)\n",
    "        else:\n",
    "            data = pd.concat([data, pd.read_csv(base_path + file, nrows=rows, low_memory=False)], axis=0)\n",
    "\n",
    "    data.drop_duplicates(subset=['case_id'], keep='first', inplace=True)\n",
    "    base_data = base_data.merge(data, on='case_id', how='left')\n",
    "\n",
    "    return merge_duplicate_group_cols(base_data)\n",
    "\n",
    "\n",
    "def get_file_names(file_names, keyword):\n",
    "    return [x for x in file_names if keyword in x]\n",
    "\n",
    "\n",
    "nrows = None\n",
    "files_path = 'csv_files/train/'\n",
    "\n",
    "files = os.listdir(files_path)\n",
    "\n",
    "base_file = ['train_base.csv']\n",
    "applprev_files = get_file_names(files, 'applprev_1')\n",
    "credit_a1_files = get_file_names(files, 'credit_bureau_a_1')\n",
    "credit_a2_files = get_file_names(files, 'credit_bureau_a_2')\n",
    "credit_b_files = get_file_names(files, 'credit_bureau_b')\n",
    "static0_files = get_file_names(files, 'static_0')\n",
    "rest_of_files = set(files) - set(applprev_files + credit_a1_files + credit_a2_files + credit_b_files +\n",
    "                                 static0_files + base_file)\n",
    "\n",
    "appl_features = ['creationdate_885D']\n",
    "credit_a1_features = ['numberofoverdueinstlmax_1039L', 'financialinstitution_591M', 'dpdmaxdateyear_596T',\n",
    "                      'dateofcredstart_739D', 'dateofcredend_289D', 'lastupdate_1112D']\n",
    "static0_features = ['price_1097A', 'isbidproduct_1095L', 'numinstlswithdpd10_728L', 'lastapprdate_640D',\n",
    "                    'lastactivateddate_801D', 'mobilephncnt_593L', 'pmtnum_254L']\n",
    "\n",
    "rest_of_features = ['incometype_1044T', 'birth_259D', 'dateofbirth_337D', 'registaddr_zipcode_184M', 'sex_738L',\n",
    "                    'contaddr_zipcode_807M']\n",
    "\n",
    "base = pd.read_csv(files_path + base_file[0], nrows=nrows)\n",
    "base = concat_and_merge(base, applprev_files, files_path, nrows, appl_features)\n",
    "base = concat_and_merge(base, credit_a1_files, files_path, nrows, credit_a1_features)\n",
    "base = concat_and_merge(base, static0_files, files_path, nrows, static0_features)\n",
    "\n",
    "for file in rest_of_files:\n",
    "    data = pd.read_csv(files_path + file, nrows=nrows, low_memory=False)\n",
    "    data.drop_duplicates(subset=['case_id'], keep='first', inplace=True)\n",
    "    base = base.merge(data, on='case_id', how='left')\n",
    "    base = merge_duplicate_group_cols(base)\n",
    "\n",
    "del data\n",
    "\n",
    "all_features = ['case_id', 'target', 'num_group1', 'date_decision'] + appl_features+credit_a1_features+static0_features+rest_of_features\n",
    "\n",
    "base = base.drop(columns=base.columns.difference(all_features))\n",
    "\n",
    "columns_to_fit = base.columns.drop(['case_id', 'target', 'num_group1', 'date_decision']).tolist()\n",
    "\n",
    "object_cols = base[columns_to_fit].select_dtypes(include=['object']).columns\n",
    "object_cols = object_cols.tolist()\n",
    "\n",
    "encoders = dict()\n",
    "for col in object_cols:\n",
    "    le = LabelEncoder()\n",
    "    base[col] = le.fit_transform(base[col])\n",
    "    encoders[col] = le\n",
    "\n",
    "lgb = LGBMClassifier(n_estimators=5000, scale_pos_weight=0.033)\n",
    "lgb.fit(base[columns_to_fit], base['target'], categorical_feature=object_cols)\n",
    "\n",
    "del base"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mek\\AppData\\Local\\Temp\\ipykernel_34808\\2380889469.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  submission.loc[:, 'score'] = lgb.predict_proba(base[columns_to_fit])[:, 1]\n"
     ]
    }
   ],
   "source": [
    "#==============================================\n",
    "# test\n",
    "#==============================================\n",
    "\n",
    "nrows = None\n",
    "files_path = 'csv_files/test/'\n",
    "\n",
    "files = os.listdir(files_path)\n",
    "\n",
    "base_file = ['test_base.csv']\n",
    "applprev_files = get_file_names(files, 'applprev_1')\n",
    "credit_a1_files = get_file_names(files, 'credit_bureau_a_1')\n",
    "credit_a2_files = get_file_names(files, 'credit_bureau_a_2')\n",
    "credit_b_files = get_file_names(files, 'credit_bureau_b')\n",
    "static0_files = get_file_names(files, 'static_0')\n",
    "rest_of_files = set(files) - set(applprev_files + credit_a1_files + credit_a2_files + credit_b_files +\n",
    "                                 static0_files + base_file)\n",
    "\n",
    "base = pd.read_csv(files_path + base_file[0], nrows=nrows)\n",
    "base = concat_and_merge(base, applprev_files, files_path, nrows, appl_features)\n",
    "base = concat_and_merge(base, credit_a1_files, files_path, nrows, credit_a1_features)\n",
    "base = concat_and_merge(base, static0_files, files_path, nrows, static0_features)\n",
    "\n",
    "for file in rest_of_files:\n",
    "    data = pd.read_csv(files_path + file, nrows=nrows, low_memory=False)\n",
    "    data.drop_duplicates(subset=['case_id'], keep='first', inplace=True)\n",
    "    base = base.merge(data, on='case_id', how='left')\n",
    "    base = merge_duplicate_group_cols(base)\n",
    "\n",
    "del data\n",
    "\n",
    "base = base.drop(columns=base.columns.difference(all_features))\n",
    "\n",
    "for col in object_cols:\n",
    "    base[col] = base[col].map(lambda s: 'unknown' if s not in encoders[col].classes_ else s)\n",
    "    encoders[col].classes_ = np.append(encoders[col].classes_, 'unknown')  # Add 'unknown' to classes\n",
    "    base[col] = encoders[col].transform(base[col])\n",
    "\n",
    "submission = base[['case_id']]\n",
    "submission.loc[:, 'score'] = lgb.predict_proba(base[columns_to_fit])[:, 1]\n",
    "submission = submission.set_index('case_id')\n",
    "submission.to_csv('./submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T14:01:18.402746Z",
     "start_time": "2024-04-07T14:01:18.318466Z"
    }
   },
   "id": "7b80a1a33d4d02fb",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T14:01:18.406512Z",
     "start_time": "2024-04-07T14:01:18.403754Z"
    }
   },
   "id": "e5157ceb9f88e721",
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
