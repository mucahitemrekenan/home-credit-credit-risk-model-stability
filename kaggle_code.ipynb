{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-25T17:12:24.083102Z",
     "start_time": "2024-05-25T17:12:23.101665Z"
    }
   },
   "source": [
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier, DMatrix\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print('import done')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T17:12:24.119831Z",
     "start_time": "2024-05-25T17:12:24.085109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VotingModel(BaseEstimator, RegressorMixin):\n",
    "\tdef __init__(self, estimators):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.estimators = estimators\n",
    "\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\treturn self\n",
    "\n",
    "\tdef predict(self, X):\n",
    "\t\ty_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "\t\treturn np.mean(y_preds, axis=0)\n",
    "\n",
    "\tdef predict_proba(self, X):\n",
    "\t\ty_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n",
    "\t\treturn np.mean(y_preds, axis=0)\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "\t@staticmethod\n",
    "\tdef set_table_dtypes(df):\n",
    "\t\tfor col in df.columns:\n",
    "\t\t\tif col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "\t\t\t\tdf = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "\t\t\telif col in [\"date_decision\"]:\n",
    "\t\t\t\tdf = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "\t\t\telif col[-1] in (\"P\", \"A\"):\n",
    "\t\t\t\tdf = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "\t\t\telif col[-1] in (\"M\",):\n",
    "\t\t\t\tdf = df.with_columns(pl.col(col).cast(pl.String))\n",
    "\t\t\telif col[-1] in (\"D\",):\n",
    "\t\t\t\tdf = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "\n",
    "\t\treturn df\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef handle_dates(df):\n",
    "\t\tfor col in df.columns:\n",
    "\t\t\tif col[-1] in (\"D\",):\n",
    "\t\t\t\tdf = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
    "\t\t\t\tdf = df.with_columns(pl.col(col).dt.total_days())\n",
    "\n",
    "\t\tdf = df.drop(\"date_decision\", \"MONTH\")\n",
    "\n",
    "\t\treturn df\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef filter_cols(df):\n",
    "\t\tfor col in df.columns:\n",
    "\t\t\tif col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "\t\t\t\tisnull = df[col].is_null().mean()\n",
    "\n",
    "\t\t\t\tif isnull > 0.95:\n",
    "\t\t\t\t\tdf = df.drop(col)\n",
    "\n",
    "\t\tfor col in df.columns:\n",
    "\t\t\tif (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "\t\t\t\tfreq = df[col].n_unique()\n",
    "\n",
    "\t\t\t\tif (freq == 1) | (freq > 200):\n",
    "\t\t\t\t\tdf = df.drop(col)\n",
    "\n",
    "\t\treturn df\n",
    "\n",
    "\n",
    "class Aggregator:\n",
    "\t@staticmethod\n",
    "\tdef num_expr(df):\n",
    "\t\tcols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "\n",
    "\t\texpr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "\t\treturn expr_max\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef date_expr(df):\n",
    "\t\tcols = [col for col in df.columns if col[-1] in (\"D\",)]\n",
    "\n",
    "\t\texpr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "\t\treturn expr_max\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef str_expr(df):\n",
    "\t\tcols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "\n",
    "\t\texpr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "\t\treturn expr_max\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef other_expr(df):\n",
    "\t\tcols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "\n",
    "\t\texpr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "\t\treturn expr_max\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef count_expr(df):\n",
    "\t\tcols = [col for col in df.columns if \"num_group\" in col]\n",
    "\n",
    "\t\texpr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "\t\treturn expr_max\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef get_exprs(df):\n",
    "\t\texprs = Aggregator.num_expr(df) + \\\n",
    "\t\t\t\tAggregator.date_expr(df) + \\\n",
    "\t\t\t\tAggregator.str_expr(df) + \\\n",
    "\t\t\t\tAggregator.other_expr(df) + \\\n",
    "\t\t\t\tAggregator.count_expr(df)\n",
    "\n",
    "\t\treturn exprs\n",
    "\n",
    "\n",
    "def read_file(path, depth=None):\n",
    "\tdf = pl.read_parquet(path)\n",
    "\tdf = df.pipe(Pipeline.set_table_dtypes)\n",
    "\n",
    "\tif depth in [1, 2]:\n",
    "\t\tdf = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "\n",
    "\treturn df\n",
    "\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "\tchunks = []\n",
    "\tfor path in glob(str(regex_path)):\n",
    "\t\tchunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes))\n",
    "\n",
    "\tdf = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "\tif depth in [1, 2]:\n",
    "\t\tdf = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "\n",
    "\treturn df\n",
    "\n",
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "\tdf_base = (\n",
    "\t\tdf_base\n",
    "\t\t.with_columns(\n",
    "\t\t\tmonth_decision=pl.col(\"date_decision\").dt.month(),\n",
    "\t\t\tweekday_decision=pl.col(\"date_decision\").dt.weekday(),\n",
    "\t\t)\n",
    "\t)\n",
    "\n",
    "\tfor i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "\t\tdf_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "\n",
    "\tdf_base = df_base.pipe(Pipeline.handle_dates)\n",
    "\n",
    "\treturn df_base\n",
    "\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "\tdf_data = df_data.to_pandas()\n",
    "\n",
    "\tif cat_cols is None:\n",
    "\t\tcat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "\n",
    "\tdf_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "\n",
    "\treturn df_data, cat_cols\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "\t\"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "\tstart_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "\tprint('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "\tfor col in df.columns:\n",
    "\t\tcol_type = df[col].dtype\n",
    "\t\tif str(col_type) == \"category\":\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tif col_type != object:\n",
    "\t\t\tc_min = df[col].min()\n",
    "\t\t\tc_max = df[col].max()\n",
    "\t\t\tif str(col_type)[:3] == 'int':\n",
    "\t\t\t\tif c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "\t\t\t\t\tdf[col] = df[col].astype(np.int8)\n",
    "\t\t\t\telif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "\t\t\t\t\tdf[col] = df[col].astype(np.int16)\n",
    "\t\t\t\telif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "\t\t\t\t\tdf[col] = df[col].astype(np.int32)\n",
    "\t\t\t\telif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "\t\t\t\t\tdf[col] = df[col].astype(np.int64)\n",
    "\t\t\telse:\n",
    "\t\t\t\tif c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "\t\t\t\t\tdf[col] = df[col].astype(np.float16)\n",
    "\t\t\t\telif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "\t\t\t\t\tdf[col] = df[col].astype(np.float32)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdf[col] = df[col].astype(np.float64)\n",
    "\t\telse:\n",
    "\t\t\tcontinue\n",
    "\tend_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "\tprint('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "\tprint('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "\treturn df\n",
    "\n",
    "\n",
    "print('functions done')"
   ],
   "id": "4ffd75dc87d3f852",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions done\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T17:12:54.088331Z",
     "start_time": "2024-05-25T17:12:24.120346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ROOT = Path(\"D:/projects\\home-credit-credit-risk-model-stability\")\n",
    "TRAIN_DIR = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "data_store = {\n",
    "\t\"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "\t\"depth_0\": [\n",
    "\t\tread_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "\t\tread_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "\t],\n",
    "\t\"depth_1\": [\n",
    "\t\tread_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "\t\tread_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "\t\tread_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "\t\tread_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "\t\tread_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "\t\tread_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "\t\tread_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "\t\tread_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "\t\tread_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "\t],\n",
    "\t\"depth_2\": [\n",
    "\t\tread_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "\t]\n",
    "}\n",
    "\n",
    "df_train = feature_eng(**data_store)\n",
    "print(\"train data shape:\\t\", df_train.shape)\n",
    "df_train = df_train.pipe(Pipeline.filter_cols)\n",
    "print(\"train data shape:\\t\", df_train.shape)\n",
    "df_train, cat_cols = to_pandas(df_train)\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "del data_store\n",
    "gc.collect()\n",
    "print('data read feat eng done')"
   ],
   "id": "7a56c345bf6c9eee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:\t (1526659, 376)\n",
      "train data shape:\t (1526659, 267)\n",
      "Memory usage of dataframe is 2520.25 MB\n",
      "Memory usage after optimization is: 837.19 MB\n",
      "Decreased by 66.8%\n",
      "data read feat eng done\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T17:12:54.096473Z",
     "start_time": "2024-05-25T17:12:54.090339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cols_to_drop = ['lastrejectreason_759M',\n",
    "\t\t\t\t'maritalst_893M',\n",
    "\t\t\t\t'birthdate_574D',\n",
    "\t\t\t\t'max_empl_industry_691L',\n",
    "\t\t\t\t'max_rejectreasonclient_4145042M',\n",
    "\t\t\t\t'max_education_927M',\n",
    "\t\t\t\t'max_dateactivated_425D',\n",
    "\t\t\t\t'lastrejectcommoditycat_161M',\n",
    "\t\t\t\t'max_empladdr_district_926M',\n",
    "\t\t\t\t'max_recorddate_4527225D',\n",
    "\t\t\t\t'education_88M',\n",
    "\t\t\t\t'isdebitcard_729L',\n",
    "\t\t\t\t'max_empladdr_zipcode_114M',\n",
    "\t\t\t\t'requesttype_4525192L',\n",
    "\t\t\t\t'max_isdebitcard_527L',\n",
    "\t\t\t\t'max_inittransactioncode_279L',\n",
    "\t\t\t\t'max_approvaldate_319D',\n",
    "\t\t\t\t'max_housetype_905L',\n",
    "\t\t\t\t'max_empl_employedtotal_800L',\n",
    "\t\t\t\t'max_creationdate_885D',\n",
    "\t\t\t\t'cardtype_51L',\n",
    "\t\t\t\t'max_education_1138M',\n",
    "\t\t\t\t'lastrejectreasonclient_4145040M',\n",
    "\t\t\t\t'lastdelinqdate_224D',\n",
    "\t\t\t\t'max_postype_4733339M',\n",
    "\t\t\t\t'max_relationshiptoclient_642T',\n",
    "\t\t\t\t'max_contaddr_matchlist_1032L',\n",
    "\t\t\t\t'max_relationshiptoclient_415T',\n",
    "\t\t\t\t'paytype_783L',\n",
    "\t\t\t\t'max_isbidproduct_390L',\n",
    "\t\t\t\t'max_familystate_447L',\n",
    "\t\t\t\t'lastrejectdate_50D',\n",
    "\t\t\t\t'max_dtlastpmt_581D',\n",
    "\t\t\t\t'max_cancelreason_3545846M',\n",
    "\t\t\t\t'max_rejectreason_755M',\n",
    "\t\t\t\t'bankacctype_710L',\n",
    "\t\t\t\t'max_contaddr_smempladdr_334L',\n",
    "\t\t\t\t'max_credacc_status_367L',\n",
    "\t\t\t\t'lastcancelreason_561M',\n",
    "\t\t\t\t'lastrejectcommodtypec_5251769M',\n",
    "\t\t\t\t'max_remitter_829L',\n",
    "\t\t\t\t'typesuite_864L',\n",
    "\t\t\t\t'dtlastpmtallstes_4499206D',\n",
    "\t\t\t\t'education_1103M']"
   ],
   "id": "2effc48751df0e14",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-25T17:12:54.098491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = df_train.drop(columns=cols_to_drop, errors='ignore')\n",
    "X = df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n",
    "y = df_train[\"target\"]\n",
    "weeks = df_train[\"WEEK_NUM\"]\n",
    "cols_to_fit = df_train.columns\n",
    "print('training data ready')\n",
    "del df_train\n",
    "print('df_train deleted')\n",
    "\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n",
    "\n",
    "params = {\n",
    "\t\"boosting_type\": \"gbdt\",\n",
    "\t\"objective\": \"binary\",\n",
    "\t\"metric\": \"auc\",\n",
    "\t\"max_depth\": -1,\n",
    "\t\"learning_rate\": 0.05,\n",
    "\t\"n_estimators\": 1000,\n",
    "\t\"colsample_bytree\": 0.8,\n",
    "\t\"colsample_bynode\": 0.8,\n",
    "\t\"verbose\": -1,\n",
    "\t\"random_state\": 42,\n",
    "\t\"device\": \"cpu\"\n",
    "}\n",
    "fitted_models = []\n",
    "\n",
    "for idx_train, idx_valid in tqdm(cv.split(X, y, groups=weeks)):\n",
    "\tmodel = LGBMClassifier(**params)\n",
    "\tmodel.fit(\n",
    "\t\tX.iloc[idx_train], y.iloc[idx_train],\n",
    "\t\teval_set=[(X.iloc[idx_valid], y.iloc[idx_valid])]\n",
    "\t)\n",
    "\tfitted_models.append(model)\n",
    "model = VotingModel(fitted_models)\n",
    "print('all models trained')\n",
    "del X, y\n",
    "gc.collect()\n",
    "print('data deleted')"
   ],
   "id": "9373258aaf383b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data ready\n",
      "df_train deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:23, 83.28s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "data_store = {\n",
    "\t\"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n",
    "\t\"depth_0\": [\n",
    "\t\tread_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "\t\tread_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "\t],\n",
    "\t\"depth_1\": [\n",
    "\t\tread_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "\t\tread_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "\t\tread_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "\t\tread_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "\t\tread_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "\t\tread_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "\t\tread_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "\t\tread_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "\t\tread_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "\t],\n",
    "\t\"depth_2\": [\n",
    "\t\tread_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "\t]\n",
    "}\n",
    "\n",
    "df_test = feature_eng(**data_store)\n",
    "print(\"test data shape:\\t\", df_test.shape)\n",
    "df_test = df_test.select([col for col in cols_to_fit if col != \"target\"])\n",
    "print(\"test data shape:\\t\", df_test.shape)\n",
    "df_test, cat_cols = to_pandas(df_test, cat_cols)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "del data_store\n",
    "gc.collect()\n",
    "\n",
    "X_test = df_test.drop(columns=[\"WEEK_NUM\"])\n",
    "X_test = X_test.set_index(\"case_id\")\n",
    "\n",
    "y_pred = pd.Series(model.predict_proba(X_test)[:, 1], index=X_test.index)"
   ],
   "id": "f6eeaa6bd6952c62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "\n",
    "df_subm[\"score\"] = y_pred\n",
    "df_subm.to_csv(\"submission.csv\")"
   ],
   "id": "6367cd6479135ee",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
