{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-24T21:17:02.337643Z",
     "start_time": "2024-05-24T21:17:02.039467Z"
    }
   },
   "source": [
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold, ParameterGrid\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier, DMatrix\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "print('import done')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T21:17:02.360388Z",
     "start_time": "2024-05-24T21:17:02.338650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VotingModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, estimators):\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "    \n",
    "\n",
    "class Pipeline:\n",
    "    @staticmethod\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))            \n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())\n",
    "                \n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_cols(df):\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "\n",
    "                if isnull > 0.95:\n",
    "                    df = df.drop(col)\n",
    "\n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "\n",
    "                if (freq == 1) | (freq > 200):\n",
    "                    df = df.drop(col)\n",
    "\n",
    "        return df\n",
    "    \n",
    "\n",
    "class Aggregator:\n",
    "    @staticmethod\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\",)]\n",
    "\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        \n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        \n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "\n",
    "        return exprs\n",
    "    \n",
    "\n",
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    \n",
    "    if depth in [1, 2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    for path in glob(str(regex_path)):\n",
    "        chunks.append(pl.read_parquet(path).pipe(Pipeline.set_table_dtypes))\n",
    "        \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    if depth in [1, 2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "        \n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    \n",
    "    return df_base\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    \n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    \n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    \n",
    "    return df_data, cat_cols\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "print('functions done')"
   ],
   "id": "4ffd75dc87d3f852",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions done\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T21:17:21.430813Z",
     "start_time": "2024-05-24T21:17:02.360900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ROOT            = Path(\"D:/projects\\home-credit-credit-risk-model-stability\")\n",
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_train = feature_eng(**data_store)\n",
    "print(\"train data shape:\\t\", df_train.shape)\n",
    "df_train = df_train.pipe(Pipeline.filter_cols)\n",
    "print(\"train data shape:\\t\", df_train.shape)\n",
    "df_train, cat_cols = to_pandas(df_train)\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "del data_store\n",
    "gc.collect()\n",
    "\n",
    "print('data read feat eng done')"
   ],
   "id": "7a56c345bf6c9eee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape:\t (1526659, 376)\n",
      "train data shape:\t (1526659, 267)\n",
      "Memory usage of dataframe is 2520.25 MB\n",
      "Memory usage after optimization is: 837.19 MB\n",
      "Decreased by 66.8%\n",
      "data read feat eng done\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-24T21:17:21.431817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n",
    "y = df_train[\"target\"]\n",
    "weeks = df_train[\"WEEK_NUM\"]\n",
    "cols_to_fit = df_train.columns\n",
    "print('training data ready')\n",
    "#del df_train\n",
    "print('df_train deleted')\n",
    "\n",
    "cv = StratifiedGroupKFold(n_splits=10, shuffle=False)\n",
    "\n",
    "param_grid = {\n",
    "\t'n_estimators': [1500, 2000],\n",
    "\t'learning_rate': [0.05],\n",
    "\t'scale_pos_weight': [1, 15, 30, 50],\n",
    "\t'max_depth': [-1, 5, 8],\n",
    "    \"objective\": [\"binary\"],\n",
    "    \"metric\": [\"auc\"],\n",
    "    \"colsample_bytree\": [0.8, 1], \n",
    "    \"colsample_bynode\": [0.8, 1],\n",
    "    \"verbose\": [-1],\n",
    "    \"random_state\": [42],\n",
    "    \"device\": [\"cpu\"]\n",
    "}\n",
    "results_df = pd.DataFrame()\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "for params in tqdm(grid):\n",
    "    results = list()\n",
    "    fitted_models = []\n",
    "\n",
    "    for idx_train, idx_valid in tqdm(cv.split(X, y, groups=weeks)):\n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X.iloc[idx_train], y.iloc[idx_train],\n",
    "            eval_set=[(X.iloc[idx_valid], y.iloc[idx_valid])]\n",
    "        )\n",
    "        fitted_models.append(model)\n",
    "    model = VotingModel(fitted_models)\n",
    "    preds = np.argmax(model.predict_proba(X), axis=1)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y, preds).ravel()\n",
    "    params.update({'tn':tn, 'fp':fp, 'fn':fn, 'tp':tp})\n",
    "    results.append(params)\n",
    "    results_df = pd.concat([results_df, pd.DataFrame(results)])\n",
    "    results_df.to_excel('grid_search.xlsx', index=False)\n"
   ],
   "id": "9373258aaf383b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data ready\n",
      "df_train deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/96 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [01:20, 80.00s/it]\u001B[A\n",
      "2it [02:39, 79.64s/it]\u001B[A\n",
      "3it [03:58, 79.36s/it]\u001B[A\n",
      "4it [05:17, 79.22s/it]\u001B[A\n",
      "5it [06:36, 79.20s/it]\u001B[A\n",
      "6it [07:56, 79.36s/it]\u001B[A\n",
      "7it [09:15, 79.47s/it]\u001B[A\n",
      "8it [10:34, 79.13s/it]\u001B[A\n",
      "9it [11:54, 79.38s/it]\u001B[A\n",
      "10it [13:14, 79.45s/it]\u001B[A\n",
      "  1%|          | 1/96 [15:09<23:59:53, 909.40s/it]\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [01:27, 87.28s/it]\u001B[A\n",
      "2it [02:53, 86.54s/it]\u001B[A\n",
      "3it [04:18, 85.92s/it]\u001B[A\n",
      "4it [06:08, 95.62s/it]\u001B[A\n",
      "5it [07:34, 92.00s/it]\u001B[A\n",
      "6it [09:01, 90.28s/it]\u001B[A\n",
      "7it [10:25, 88.16s/it]\u001B[A\n",
      "8it [11:50, 87.11s/it]\u001B[A\n",
      "9it [13:17, 87.07s/it]\u001B[A\n",
      "10it [14:44, 88.43s/it]\u001B[A\n",
      "  2%|▏         | 2/96 [31:57<25:15:32, 967.37s/it]\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [01:51, 111.05s/it]\u001B[A\n",
      "2it [03:18, 96.90s/it] \u001B[A\n",
      "3it [04:44, 91.99s/it]\u001B[A\n",
      "4it [06:10, 89.57s/it]\u001B[A\n",
      "5it [07:36, 88.38s/it]\u001B[A\n",
      "6it [09:02, 87.67s/it]\u001B[A\n",
      "7it [10:27, 86.88s/it]\u001B[A\n",
      "8it [11:52, 86.05s/it]\u001B[A\n",
      "9it [13:16, 85.59s/it]\u001B[A\n",
      "10it [14:44, 88.40s/it]\u001B[A\n",
      "  3%|▎         | 3/96 [48:46<25:29:20, 986.67s/it]\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [01:29, 89.03s/it]\u001B[A\n",
      "2it [02:56, 87.91s/it]\u001B[A\n",
      "3it [04:24, 88.17s/it]\u001B[A\n",
      "4it [05:51, 87.49s/it]\u001B[A\n",
      "5it [07:16, 86.74s/it]\u001B[A\n",
      "6it [08:43, 86.73s/it]\u001B[A\n",
      "7it [10:09, 86.53s/it]\u001B[A\n",
      "8it [11:34, 86.22s/it]\u001B[A\n",
      "9it [13:00, 86.13s/it]\u001B[A\n",
      "10it [14:26, 86.67s/it]\u001B[A\n",
      "  4%|▍         | 4/96 [1:05:17<25:15:15, 988.21s/it]\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [01:40, 100.84s/it]\u001B[A\n",
      "2it [03:21, 100.76s/it]\u001B[A\n",
      "3it [05:02, 101.08s/it]\u001B[A\n",
      "4it [06:41, 100.25s/it]\u001B[A\n",
      "5it [08:23, 100.82s/it]\u001B[A\n",
      "6it [10:04, 100.62s/it]\u001B[A\n",
      "7it [11:44, 100.53s/it]\u001B[A\n",
      "8it [13:24, 100.38s/it]\u001B[A\n",
      "9it [15:31, 108.80s/it]\u001B[A\n",
      "10it [17:12, 103.21s/it]\u001B[A\n",
      "  5%|▌         | 5/96 [1:24:48<26:38:37, 1054.04s/it]\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [01:47, 107.14s/it]\u001B[A\n",
      "2it [03:34, 107.34s/it]\u001B[A\n",
      "3it [05:19, 106.34s/it]\u001B[A\n",
      "4it [07:07, 106.72s/it]\u001B[A\n",
      "5it [08:52, 106.35s/it]\u001B[A\n",
      "6it [10:37, 105.90s/it]\u001B[A\n",
      "7it [12:21, 105.25s/it]\u001B[A\n",
      "8it [14:07, 105.47s/it]\u001B[A\n",
      "9it [15:54, 105.81s/it]\u001B[A\n",
      "10it [17:42, 106.24s/it]\u001B[A\n",
      "  6%|▋         | 6/96 [1:44:58<27:40:31, 1107.02s/it]\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [01:47, 107.55s/it]\u001B[A\n",
      "2it [03:34, 106.98s/it]\u001B[A\n",
      "3it [05:21, 107.21s/it]\u001B[A\n",
      "4it [07:08, 107.08s/it]\u001B[A\n",
      "5it [08:55, 107.04s/it]\u001B[A\n",
      "6it [10:43, 107.41s/it]\u001B[A\n",
      "7it [12:29, 106.99s/it]\u001B[A\n",
      "8it [14:15, 106.57s/it]\u001B[A"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
